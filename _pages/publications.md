---
permalink: /publications/
title: "Publications"
---
{::options parse_block_html="true" /}

## Preprints
Can't spoiler them yet! :eyes:

## 2025
**Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering**
--
Francesco Maria Molfese, Luca Moroni, *Luca Gioffré*, Alessandro Scirè, Simone Conia, Roberto Navigli 
-- 
ACL Findings 2025  
[![arXiv](https://img.shields.io/badge/arXiv-paper-b31b1b.svg)](https://arxiv.org/abs/2503.14996) 
[![Hugging Face Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-FCD21D)](https://huggingface.co/datasets/sapienzanlp/MMLU-Adversarial) 
[![GitHub](https://img.shields.io/badge/GitHub-Official Repository-blue)](https://github.com/Andrew-Wyn/metaQAeval)
[![post](https://img.shields.io/badge/Blog-Presentation Post-green)]({% link _posts/2025-03-19-RAWS-preprint.md %}) 

<details>

*Traditional MCQA evaluation strategies often underestimate LLM capabilities, while LLM-based answer extractors are prone to systematic errors.* 
*Moreover, there exists a fundamental trade-off between constraining a model's output to simplify answer extraction and allowing it to freely generate to improve reasoning.* 
*These findings call for standardized evaluation methodologies and highlight the need for more reliable and consistent MCQA evaluation practices.*

</details>

---
*[LLM]: Large Language Model
*[MCQA]: Multiple-Choice Question Answering